{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建结构化的tensorflow代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下内容严重借鉴于这篇博客[传送门](https://danijar.com/structuring-your-tensorflow-models/), 墙裂建议阅读原文."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 菜鸟级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说, 我们自己直接用的脚本都比较简单直接, 写几个函数, 按顺序调用就行了. 如下所示面的代码所示, 下面的代码出自一下链接:\n",
    "[传送门](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_raw.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\" Neural Network.\n",
    "A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\n",
    "implementation with TensorFlow. This example is using the MNIST database\n",
    "of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n",
    "Links:\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码是否觉得十分熟悉? 反正我自己最开始入门的时候就是学习这样自的代码的. 简单易容, 脚本感强烈. 然而, 一开始看别人的论文中实现的一些库就稍微有点蛋疼了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向结构化转型中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在慢慢地探索过程中, 就知道了将代码结构话还是很有必要的, 代码的复用行会好很多. 所以, 我们可以不妨尝试着将代码成类的形式:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        data_size        = int(data.get_shape()[1])\n",
    "        target_size      = int(target.get_shape()[1])\n",
    "        \n",
    "        weight           = tf.Variable(tf.truncated_normal([data_size, target_size]))\n",
    "        bias             = tf.Variable(tf.constant(0.1, shape=[target_size]))\n",
    "        incoming         = tf.matmul(data, weight) + bias\n",
    "        \n",
    "        self._prediction = tf.nn.softmax(incoming)\n",
    "        cross_entropy    = -tf.reduce_sum(target, tf.log(self._prediction))\n",
    "        self._optimize   = tf.train.RMSPropOptimizer(0.03).minimize(cross_entropy)\n",
    "        \n",
    "        mistakes         = tf.not_equal(\n",
    "                              tf.argmax(target, 1), tf.argmax(self._prediction, 1))\n",
    "        self._error      = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @property\n",
    "    def prediction(self):\n",
    "        return self._prediction\n",
    "\n",
    "    @property\n",
    "    def optimize(self):\n",
    "        return self._optimize\n",
    "\n",
    "    @property\n",
    "    def error(self):\n",
    "        return self._error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码就是类的形式了. 我们在初始话的时候, 就将整个网络的构建好, 将其属性定义好, 使用的时候就用调用属性的方法来获取相应的结果就行了.美滋滋.\n",
    "\n",
    "其中**_`@property`_**是什么意思呢, 这在python中定义为一个装饰器, 涉及到了函数式编程的思想, 在这里的作用呢, 就是将函数的返回值当成了一个属性值. 如下所示, 接下来我还会继续详细地解释一下这个装饰器的内容, 现在知道这里就可以了:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "network    = Model(images, labels)\n",
    "prediction = network.prediciton\n",
    "error      = network.error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实, 上面的代码一看就知道里面还是存在这很多问题的, 最突出的一点就是将所有的定义都放在到了__init__函数中, 一点都不优雅, 每次使用的时候,都得重新定义一遍, 可复用性极低, 和上面的脚本没有太大额区别, 只是披了一个类的外壳而已."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对类进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以, 我们正确的解锁姿势应该是网络的构建, 预测和错误分析都放进各自的函数中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._prediction = None\n",
    "        self._optimize = None\n",
    "        self._error = None\n",
    "\n",
    "    @property\n",
    "    def prediction(self):\n",
    "        if not self._prediction:\n",
    "            data_size = int(self.data.get_shape()[1])\n",
    "            target_size = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal([data_size, target_size]))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[target_size]))\n",
    "            incoming = tf.matmul(self.data, weight) + bias\n",
    "            self._prediction = tf.nn.softmax(incoming)\n",
    "        return self._prediction\n",
    "\n",
    "    @property\n",
    "    def optimize(self):\n",
    "        if not self._optimize:\n",
    "            cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction))\n",
    "            optimizer = tf.train.RMSPropOptimizer(0.03)\n",
    "            self._optimize = optimizer.minimize(cross_entropy)\n",
    "        return self._optimize\n",
    "\n",
    "    @property\n",
    "    def error(self):\n",
    "        if not self._error:\n",
    "            mistakes = tf.not_equal(\n",
    "                tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "            self._error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "        return self._error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的代码, 应该说还是相当优美的了, 注意到, **`self._prediction`**和**`self._optimize`**他们的定义, 初始化的时候, 前面的定义是有下划线的, 后面的函数的定义的时候, 开头是没有下划线的? 这里有什么奥妙麽, 注意结合**`@property`**这一个装饰器的作用?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 我们在正常调用的时候, 应该调用的方式如下:   \n",
    "``` Python\n",
    "    prediction = Model.prediction\n",
    "```\n",
    "\n",
    "而初始化的时候, 定义*self._prediciton*的作用是使得这些属性的构造过程只在第一次被调用的时候运行一次, 第二次的时候, 就不会再运行那几行代码了, 所以说, 这种形式还是相当又优雅的. 到这里, 我们应该还是挺满意的了. \n",
    "\n",
    "但是突然间, 你会发现*prediction*, *optimize* 以及*error*三个函数的构造基本是一毛一样的, 都是先判断, 后赋值. 好像还是有一点重复和冗余的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要叫做高级优化呢, 因为这种优化你要付出蛮多的学习成本, 甚至, 你会觉得不用做这些优化都可以了, 上面的冗余自己可以接受. 但是, 我们得看懂被人的代码呀, 所以我们还是不妨先看一看吧.   \n",
    "这里用到的关键技术就是`python`里面美丽的装饰器. 装饰器的内容详细解说, 我又要给大家强力安利[**Python cookbook 3rd**](http://python3-cookbook.readthedocs.io/zh_CN/latest/chapters/p09_meta_programming.html). 这里面一个最重要的思想就是函数式编程的思想, 就是函数成为了一等公民, 函数名可以直接用来当做参数进行传递. 就像C语言中的函数指针指向函数定义的首地址, 可以用来传递一样. 函数式编程的另外一个重要的体现就是lambda表达式, 在此提一下不进行分析.\n",
    "\n",
    "下面我们先来看一个Python装饰器的简单应用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timethis(func):\n",
    "    '''\n",
    "    Decorator that reports the execution time.\n",
    "    '''\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(func.__name__, end-start)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "上面定义了一个简单的装饰器, 这个装饰器可以将被修饰的函数的运行时间打印出来."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countdown 0.0062198638916015625\n",
      "countdown 0.595980167388916\n"
     ]
    }
   ],
   "source": [
    "@timethis\n",
    "def countdown(n):\n",
    "     '''\n",
    "     Counts down\n",
    "     '''\n",
    "     while n > 0:\n",
    "         n -= 1\n",
    "\n",
    "countdown(100000)\n",
    "countdown(10000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟像下面这样写其实效果是一样的：\n",
    "\n",
    "顺便说一下，内置的装饰器比如 @staticmethod, @classmethod,@property 原理也是一样的。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countdown 0.006100893020629883\n",
      "countdown 0.5906565189361572\n"
     ]
    }
   ],
   "source": [
    "def countdown(n):\n",
    "     '''\n",
    "     Counts down\n",
    "     '''\n",
    "     while n > 0:\n",
    "         n -= 1\n",
    "\n",
    "countdown = timethis(countdown)\n",
    "countdown(100000)\n",
    "countdown(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以, 从上面我们就可以看到, 装饰器其实就是接受一个函数作为输入参数, 然后给调用这返回一个新的被重新构造过的函数, 来给函数添加一些通用的功能. 所以, 我们就可以利用装饰器来简化以上, 我们上面的Tensorflow代码了.\n",
    "\n",
    "先定义一个装饰器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后再重新改写class Model的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.error\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        data_size = int(self.data.get_shape()[1])\n",
    "        target_size = int(self.target.get_shape()[1])\n",
    "        weight = tf.Variable(tf.truncated_normal([data_size, target_size]))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[target_size]))\n",
    "        incoming = tf.matmul(self.data, weight) + bias\n",
    "        return tf.nn.softmax(incoming)\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction))\n",
    "        optimizer = tf.train.RMSPropOptimizer(0.03)\n",
    "        return optimizer.minimize(cross_entropy)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个世界都变得简单和明了了. 上面的惰性检查就全部被一个装饰器来替代了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再煮一个栗子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用装饰器来给我们的函数添加name_scope, 装饰器定义如下, 其具体的用法跟上面的基本一毛一样."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(function.__name):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面主要介绍了使用类来编写tensorflow模型的一个基本的模板, 然后在使用装饰器来对这个类进行优化, 既提高了代码的可读性, 又提高了代码的运行效率.   \n",
    "细心的你, 应该也可以发现, 其实上面的编写和优化和tensorflow本身并没有很强的相关性, 所以其实这个模板也完全可以套用到你喜欢的各种深度学习框架中, 以上希望能够给你带来一点参考."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-py3",
   "language": "python",
   "name": "tf-gpu-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
